{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:02.485952Z",
     "iopub.status.busy": "2025-03-23T10:28:02.485555Z",
     "iopub.status.idle": "2025-03-23T10:28:02.918029Z",
     "shell.execute_reply": "2025-03-23T10:28:02.916779Z",
     "shell.execute_reply.started": "2025-03-23T10:28:02.485891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n",
      "/kaggle/input/gemma/transformers/2b/2/config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/2b/2/.gitattributes\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b/2/generation_config.json\n",
      "/kaggle/input/santa-2024/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # For numerical computations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "\n",
    "# Importing the os module to interact with the file system\n",
    "import os\n",
    "\n",
    "# This loop will list all files in the '/kaggle/input' directory.\n",
    "# Useful for exploring the input data files available in the Kaggle environment.\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Note:\n",
    "# - You can save up to 20GB of data in the '/kaggle/working/' directory, which persists when you save your notebook.\n",
    "# - Temporary files can be written to '/kaggle/temp/', but they won't be saved after the session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:02.920597Z",
     "iopub.status.busy": "2025-03-23T10:28:02.920110Z",
     "iopub.status.idle": "2025-03-23T10:28:02.947906Z",
     "shell.execute_reply": "2025-03-23T10:28:02.946865Z",
     "shell.execute_reply.started": "2025-03-23T10:28:02.920560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  20  20  30  50 100]\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # For numerical computations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from collections import Counter  # For counting hashable objects\n",
    "from tqdm import tqdm  # For displaying progress bars in loops\n",
    "import random  # For generating random numbers\n",
    "import pickle  # For serializing and deserializing Python objects\n",
    "import math  # For mathematical operations\n",
    "import warnings  # For handling warnings\n",
    "\n",
    "# Uncomment the line below to suppress warnings (useful in some cases)\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "# Path to the input CSV file\n",
    "p = '/kaggle/input/santa-2024/sample_submission.csv'\n",
    "\n",
    "# Reading the CSV file into a DataFrame\n",
    "df = pd.read_csv(p)  # The file contains columns like 'id' and 'text'\n",
    "\n",
    "# Printing the number of words in each 'text' entry\n",
    "# The lambda function splits the text into words and counts them\n",
    "print(df['text'].map(lambda x: len(str(x).split(' '))).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:02.950060Z",
     "iopub.status.busy": "2025-03-23T10:28:02.949626Z",
     "iopub.status.idle": "2025-03-23T10:28:02.957243Z",
     "shell.execute_reply": "2025-03-23T10:28:02.956076Z",
     "shell.execute_reply.started": "2025-03-23T10:28:02.950012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "--------------------\n",
      "advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh workshop stocking holly jingle beard naughty nice sing of is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy wonder believe dream hope peace joy merry season greeting card wrapping paper bow cookie milk star wish wreath angel to in that have it not with as you from we kaggle\n"
     ]
    }
   ],
   "source": [
    "# Extracting the 'text' column values from the DataFrame\n",
    "tokens = df.text.values\n",
    "\n",
    "# Joining all text entries into a single string\n",
    "tokens = ' '.join(tokens)\n",
    "\n",
    "# Splitting the string into individual words (tokens)\n",
    "tokens = tokens.split(' ')\n",
    "\n",
    "# Counting the frequency of each unique word using Counter\n",
    "tokens = dict(Counter(tokens))\n",
    "\n",
    "# Printing the total number of unique words\n",
    "print(len(tokens))\n",
    "\n",
    "# Uncomment the line below to print the dictionary of word frequencies\n",
    "# print(tokens)\n",
    "\n",
    "# Printing a separator for better readability\n",
    "print('-' * 20)\n",
    "\n",
    "# Printing all unique words as a single space-separated string\n",
    "print(' '.join(k for k in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:02.960246Z",
     "iopub.status.busy": "2025-03-23T10:28:02.959391Z",
     "iopub.status.idle": "2025-03-23T10:28:35.299797Z",
     "shell.execute_reply": "2025-03-23T10:28:35.298441Z",
     "shell.execute_reply.started": "2025-03-23T10:28:02.960190Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Exception'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cd37b67fff4d5394c2e02f839826f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import transformers  # For working with pre-trained transformer models\n",
    "import torch  # For tensor computations and deep learning\n",
    "import gc, os, logging  # For garbage collection, environment variables, and logging\n",
    "from math import exp  # For calculating exponential values\n",
    "from typing import List, Optional, Union  # For type hinting\n",
    "\n",
    "# Setting environment variables to optimize performance\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # Limits the number of threads for parallelism\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disables parallelism in tokenizers\n",
    "\n",
    "# Setting the device to CPU or GPU based on availability\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# Path to the pre-trained model\n",
    "MODEL_PATH = \"/kaggle/input/gemma/transformers/2b/2\"\n",
    "\n",
    "# Custom exception class for errors visible to participants\n",
    "class ParticipantVisibleError(Exception):\n",
    "    print(Exception)\n",
    "    pass\n",
    "\n",
    "# Class to calculate perplexity of text using a pre-trained language model\n",
    "class PerplexityCalculator:\n",
    "    def __init__(self):\n",
    "        # Loading the tokenizer for the pre-trained model\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        \n",
    "        # Loading the pre-trained model for causal language modeling\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        \n",
    "        # Defining the loss function (CrossEntropyLoss)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Setting the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Setting the device to GPU if available, otherwise CPU\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    # Method to calculate perplexity for a single text or a list of texts\n",
    "    def get_perplexity(self, input_texts: Union[str, List[str]]) -> Union[float, List[float]]:\n",
    "        # Check if the input is a single string or a list of strings\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "        loss_list = []\n",
    "\n",
    "        # Disable gradient computation for inference\n",
    "        with torch.no_grad():\n",
    "            for text in input_texts:\n",
    "                # Adding special tokens (BOS and EOS) to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "                \n",
    "                # Tokenizing the text and converting it to tensors\n",
    "                minputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False)\n",
    "                minputs = {k: v.to(self.device) for k, v in minputs.items()}\n",
    "                \n",
    "                # Passing the inputs through the model\n",
    "                output = self.model(**minputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "                \n",
    "                # Shifting logits and labels for loss calculation\n",
    "                slogits = logits[..., :-1, :].contiguous()\n",
    "                slabels = minputs['input_ids'][..., 1:].contiguous()\n",
    "                \n",
    "                # Calculating the loss for the sequence\n",
    "                loss = self.loss_fct(slogits.view(-1, slogits.size(-1)), slabels.view(-1))\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "        # Calculating perplexity from the loss\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "# Creating an instance of the PerplexityCalculator class\n",
    "scorer = PerplexityCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:35.302365Z",
     "iopub.status.busy": "2025-03-23T10:28:35.301622Z",
     "iopub.status.idle": "2025-03-23T10:28:35.317610Z",
     "shell.execute_reply": "2025-03-23T10:28:35.316314Z",
     "shell.execute_reply.started": "2025-03-23T10:28:35.302295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 104828,  67905,  52931,   2730,  43485, 136507,   7727, 165493,\n",
       "          29138, 103360,   1513,  80108,    541,   5376,   2734,   9902,   6109,\n",
       "          44528,    573,   6284,   3354,  10084,    578,    597,   1731,  23675,\n",
       "          42768,  17196,  22867,  12083, 138763, 198447,  16621,  99946,  16573,\n",
       "           2660,  14111, 155702,  20257,  77515, 108548, 204063,  38175,  97840,\n",
       "           4866,   2800,    576,    603,   7812,   3532,  10228,    748,  14660,\n",
       "           1965, 215898,  28162,  83096,    881,   9437,   8529, 112671, 149218,\n",
       "           7815,   1312,    869,   9471,  23144,  13171,  25720,  24754,   2398,\n",
       "           7474,  12849,   5144,   4564,   6523,   4077,   7124,  10300,  46301,\n",
       "           3891,  32338,   4076,  56178,   4368,   7181,  17467,   9512,   2343,\n",
       "           6199,  58409,  22448,    577,    575,    674,    791,    665,    780,\n",
       "            675,    685,    692,    774,    783, 124555,   2315,      1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a string of holiday-related tokens (words)\n",
    "tokens = \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh workshop stocking holly jingle beard naughty nice sing of is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy wonder believe dream hope peace joy merry season greeting card wrapping paper bow cookie milk star wish wreath angel to in that have it not with as you from we kaggle\"\n",
    "\n",
    "# Adding special tokens (BOS and EOS) to the text\n",
    "text_with_special = f\"{scorer.tokenizer.bos_token}{tokens}{scorer.tokenizer.eos_token}\"\n",
    "\n",
    "# Tokenizing the text and converting it to tensors\n",
    "minputs = scorer.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "# Printing the tokenized input IDs\n",
    "minputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:35.319800Z",
     "iopub.status.busy": "2025-03-23T10:28:35.319338Z",
     "iopub.status.idle": "2025-03-23T10:28:35.344791Z",
     "shell.execute_reply": "2025-03-23T10:28:35.343608Z",
     "shell.execute_reply.started": "2025-03-23T10:28:35.319750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Dictionary containing text sequences as keys and their perplexity scores as values\n",
    "past = {\n",
    "    'reindeer mistletoe elf scrooge gingerbread chimney fireplace ornament family advent': 495.6812574407127,\n",
    "    'reindeer mistletoe elf gingerbread ornament advent family fireplace chimney sleep drive walk jump laugh give and bake the night scrooge': 514.3236249412977,\n",
    "    'magi yuletide cheer grinch carol holly jingle naughty nice polar beard sleigh chimney workshop nutcracker holiday ornament decorations gifts stocking': 327.18895045897995,\n",
    "    'sleigh of the magi yuletide cheer grinch is unwrap gifts decorations ornament holly stocking and chimney naughty nice polar beard nutcracker visit workshop eat relax carol sing holiday cheer jingle': 327.33220436011106,\n",
    "    'wreath merry have and season hohoho to you from the star of wonder workshop that it not with joy we believe hope peace fruitcake chocolate candy peppermint candle snowglobe angel poinsettia wrapping paper bow greeting card cookie milk wish dream fireplace kaggle toy doll game night puzzle eggnog as in': 222.93269021691262,\n",
    "    'decorations eggnog yuletide poinsettia fruitcake scrooge nutcracker mistletoe holly wreath gingerbread cookie snowglobe reindeer angel star merry and the season of joy and wonder and peace to you from the family of the grinch holiday cheer is not as cheer unwrap gifts laugh hohoho sing carol in sleigh drive visit chimney chimney elf naughty nice eat bake sleep dream chocolate peppermint ornament stocking fireplace fireplace advent candle wish hope give card ornament wrapping paper toy doll bow game night night puzzle candy walk jingle jump relax believe it with we have that kaggle workshop workshop polar beard milk greeting magi': 120.92163009638384\n",
    "}\n",
    "\n",
    "# Printing the number of entries in the dictionary\n",
    "print(len(past))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:28:35.346509Z",
     "iopub.status.busy": "2025-03-23T10:28:35.346092Z",
     "iopub.status.idle": "2025-03-23T10:29:44.737701Z",
     "shell.execute_reply": "2025-03-23T10:29:44.735976Z",
     "shell.execute_reply.started": "2025-03-23T10:28:35.346473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018.7141148854306\n",
      "row:  0 1350.9216564227258\n",
      "row:  0 495.6812574407127\n",
      "row:  1 2146.111641264015\n",
      "row:  1 514.3236249412977\n",
      "row:  2 872.250522619955\n",
      "row:  2 327.18895045897995\n",
      "row:  3 834.3357780659894\n",
      "row:  3 327.33220436011106\n",
      "row:  4 489.6441270405202\n",
      "row:  4 222.93269021691262\n",
      "row:  5 419.02096389937867\n",
      "row:  5 120.92163009638384\n",
      "MEAN SCORE:  334.73005958573293\n",
      "CPU times: user 2min 12s, sys: 2.65 s, total: 2min 15s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to optimize text sequences by minimizing perplexity\n",
    "def get_best_plex(df, past):\n",
    "    l = 2  # Number of iterations for shuffling and testing new sequences\n",
    "    \n",
    "    # Calculate perplexity scores for all rows in the DataFrame\n",
    "    df['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\n",
    "    print(np.mean(df['score'].values))  # Print the mean perplexity score\n",
    "    \n",
    "    # Iterate through each row in the DataFrame\n",
    "    for r in range(len(df)):\n",
    "        # Print the current row and its perplexity score\n",
    "        print(\"row: \", r, df.at[r, 'score'])\n",
    "        \n",
    "        # Split the text into words\n",
    "        t = df['text'][r].split(' ')\n",
    "        \n",
    "        # Check if the current text matches any sequence in the past dictionary\n",
    "        for k in past:\n",
    "            kp = k.split(' ')\n",
    "            if len(kp) == len(t):  # Ensure the lengths match\n",
    "                if sorted(kp) == sorted(t):  # Check if the words match (ignoring order)\n",
    "                    if df['score'][r] > past[k]:  # If a better score exists in the past dictionary\n",
    "                        df.at[r, 'score'] = past[k]\n",
    "                        df.at[r, 'text'] = k\n",
    "        \n",
    "        # Print the updated score for the current row\n",
    "        print(\"row: \", r, df.at[r, 'score'])\n",
    "        \n",
    "        # Shuffle and test new sequences\n",
    "        for i in range(l):\n",
    "            t = df['text'][r].split(' ')\n",
    "            random.shuffle(t)  # Shuffle the words\n",
    "            t = ' '.join(t)  # Join the shuffled words into a string\n",
    "            \n",
    "            # If the new sequence is not in the past dictionary\n",
    "            if t not in past:\n",
    "                s = scorer.get_perplexity(t)  # Calculate its perplexity\n",
    "                past[t] = s  # Add it to the past dictionary\n",
    "                \n",
    "                # If the new sequence has a lower perplexity score, update the DataFrame\n",
    "                if s < df['score'][r]:\n",
    "                    df.at[r, 'score'] = s\n",
    "                    df.at[r, 'text'] = t\n",
    "                    print(r, i, \"New Score: \", s, np.mean(df['score'].values))\n",
    "        \n",
    "        # Save the updated past dictionary to a file\n",
    "        with open('past0.pickle', 'wb') as f:\n",
    "            pickle.dump(past, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Print the final mean perplexity score\n",
    "    print('MEAN SCORE: ', np.mean(df['score'].values))\n",
    "    \n",
    "    # Return the updated DataFrame and past dictionary\n",
    "    return df[['id', 'text']], past\n",
    "\n",
    "# Call the function to optimize the text sequences\n",
    "df, past = get_best_plex(df, past)\n",
    "\n",
    "# Save the optimized DataFrame to a CSV file\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 6216,
     "sourceId": 11384,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
